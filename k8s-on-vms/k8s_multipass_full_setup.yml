---
- name: Provision Kubernetes Cluster VMs with Multipass and setup SSH
  hosts: localhost
  connection: local
  gather_facts: no
  vars:
    cluster_name: "{{ cluster_name | default('k8scluster') }}"
    ssh_pub_key: "{{ lookup('file', lookup('env','HOME') + '/.ssh/id_rsa.pub') }}"
    inventory_file: "./{{ cluster_name }}_inventory.ini"
  tags:
    - provision
  tasks:
    - name: Set multipass_instances variable dynamically
      set_fact:
        multipass_instances:
          - name: "{{ cluster_name }}-master"
            cpus: 2
            memory: 4G
            disk: 20G
          - name: "{{ cluster_name }}-worker1"
            cpus: 2
            memory: 2G
            disk: 15G
          - name: "{{ cluster_name }}-worker2"
            cpus: 2
            memory: 2G
            disk: 15G
          - name: "{{ cluster_name }}-worker3"
            cpus: 2
            memory: 2G
            disk: 15G

    - name: Check for existing Multipass instances
      command: multipass info {{ item.name }}
      register: mp_info
      failed_when: false
      loop: "{{ multipass_instances }}"
      loop_control:
        loop_var: item

    - name: Build list of instances to launch (if not already present)
      set_fact:
        instances_to_launch: >-
          {{
            (instances_to_launch | default([])) +
            ([multipass_instances[item.0]] if item.1.rc != 0 else [])
          }}
      with_indexed_items: "{{ mp_info.results }}"

    - name: Launch Multipass VMs
      command: multipass launch 22.04 --name {{ item.name }} --cpus {{ item.cpus }} --memory {{ item.memory }} --disk {{ item.disk }}
      loop: "{{ instances_to_launch | default([]) }}"

    - name: Extract VM names and IPs for this cluster (robust, awk, only Running)
      shell: |
        multipass list --format csv | awk -F, '/^{{ cluster_name }}-/ && $2 == "Running" && $3 ~ /[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/ {print $1 "," $3}'
      register: cluster_vm_lines

    - name: Parse Multipass IPs for dynamic inventory
      set_fact:
        multipass_ips: "{{ dict(cluster_vm_lines.stdout_lines | map('split', ',')) }}"

    - name: Debug multipass_ips and cluster_vm_lines
      debug:
        msg: 
          - "multipass_ips: {{ multipass_ips }}"
          - "cluster_vm_lines: {{ cluster_vm_lines.stdout_lines }}"

    - name: Fail if no running VMs found for this cluster
      fail:
        msg: "No running VMs found for cluster '{{ cluster_name }}'. multipass_ips is empty. Please ensure your VMs are running and named correctly."
      when: multipass_ips | length == 0

    - name: Add SSH key to all running VMs
      command: multipass exec {{ item }} -- bash -c "echo '{{ ssh_pub_key }}' >> /home/ubuntu/.ssh/authorized_keys"
      loop: "{{ multipass_ips.keys() | list }}"
      loop_control:
        loop_var: item
      ignore_errors: true

    - name: Write dynamic inventory to file
      copy:
        dest: "{{ inventory_file }}"
        content: |
          [{{ cluster_name }}_master]
          {% if multipass_ips.get(cluster_name ~ '-master', '') %}
          {{ multipass_ips.get(cluster_name ~ '-master', '') }} ansible_user=ubuntu
          {% endif %}

          [{{ cluster_name }}_workers]
          {% for worker in ['worker1', 'worker2', 'worker3'] %}
          {% if multipass_ips.get(cluster_name ~ '-' ~ worker, '') %}
          {{ multipass_ips.get(cluster_name ~ '-' ~ worker, '') }} ansible_user=ubuntu
          {% endif %}
          {% endfor %}

          [{{ cluster_name }}_all:children]
          {{ cluster_name }}_master
          {{ cluster_name }}_workers

          [all:vars]
          ansible_ssh_private_key_file={{ lookup('env','HOME') + '/.ssh/id_rsa' }}

- name: Common setup for all nodes
  hosts: "{{ cluster_name }}_all"
  become: yes
  vars:
    k8s_version: "1.28"
    containerd_version: "1.7.2"
    pod_network_cidr: "10.244.0.0/16"
    cri_endpoint: "unix:///run/containerd/containerd.sock"
  tags:
    - cluster
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install required packages
      apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
          - software-properties-common
          - net-tools
          - htop
        state: present

    - name: Disable swap
      shell: |
        swapoff -a
        sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
      changed_when: false

    - name: Load kernel modules
      modprobe:
        name: "{{ item }}"
      loop:
        - overlay
        - br_netfilter

    - name: Add kernel modules to load at boot
      lineinfile:
        path: /etc/modules-load.d/k8s.conf
        line: "{{ item }}"
        create: yes
      loop:
        - overlay
        - br_netfilter

    - name: Configure sysctl for Kubernetes
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        sysctl_file: /etc/sysctl.d/k8s.conf
        reload: yes
      loop:
        - { name: 'net.bridge.bridge-nf-call-iptables', value: '1' }
        - { name: 'net.bridge.bridge-nf-call-ip6tables', value: '1' }
        - { name: 'net.ipv4.ip_forward', value: '1' }

    - name: Add Docker GPG key
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present

    - name: Add Docker repository
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present

    - name: Install containerd
      apt:
        name: containerd.io
        state: present
        update_cache: yes

    - name: Create containerd configuration directory
      file:
        path: /etc/containerd
        state: directory
        mode: '0755'

    - name: Generate default containerd configuration (always overwrite)
      shell: containerd config default | tee /etc/containerd/config.toml

    - name: Configure containerd to use systemd cgroup driver
      replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'

    - name: Restart and enable containerd
      systemd:
        name: containerd
        state: restarted
        enabled: yes
        daemon_reload: yes

    - name: Check containerd status
      shell: systemctl status containerd --no-pager
      register: containerd_status
      ignore_errors: true

    - name: Show containerd status
      debug:
        var: containerd_status.stdout

    - name: Check crictl info
      shell: crictl --runtime-endpoint {{ cri_endpoint }} info
      register: crictl_info
      ignore_errors: true

    - name: Show crictl info
      debug:
        var: crictl_info.stdout

    - name: Add Kubernetes GPG key
      apt_key:
        url: https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key
        state: present

    - name: Add Kubernetes repository
      apt_repository:
        repo: "deb https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /"
        state: present
        filename: kubernetes

    - name: Install Kubernetes components
      apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
        state: present
        update_cache: yes

    - name: Hold Kubernetes packages at current version
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop:
        - kubelet
        - kubeadm
        - kubectl

    - name: Enable and start kubelet
      systemd:
        name: kubelet
        enabled: yes
        state: started

    - name: Configure kubelet to use correct cgroup driver
      lineinfile:
        path: /etc/default/kubelet
        line: 'KUBELET_EXTRA_ARGS=--cgroup-driver=systemd'
        create: yes
      notify: restart kubelet

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        mode: '0755'

  handlers:
    - name: restart kubelet
      systemd:
        name: kubelet
        state: restarted

- name: Initialize Kubernetes master node
  hosts: "{{ cluster_name }}_master"
  become: yes
  vars:
    pod_network_cidr: "10.244.0.0/16"
    cri_endpoint: "unix:///run/containerd/containerd.sock"
  tags:
    - cluster
  tasks:
    - name: Check if Kubernetes is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: k8s_init_stat

    - name: Preflight check for container runtime
      command: crictl --runtime-endpoint {{ cri_endpoint }} version
      register: crictl_check
      failed_when: false

    - name: Display crictl status
      debug:
        msg: "Container runtime status: {{ crictl_check.stdout if crictl_check.rc == 0 else 'Failed: ' + crictl_check.stderr }}"

    - name: Initialize Kubernetes cluster
      shell: |
        kubeadm init \
          --pod-network-cidr={{ pod_network_cidr }} \
          --apiserver-advertise-address={{ ansible_default_ipv4.address }} \
          --node-name={{ ansible_hostname }} \
          --cri-socket={{ cri_endpoint }} \
          --v=5
      when: not k8s_init_stat.stat.exists
      register: kubeadm_init

    - name: Copy kubeconfig to root user
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: yes
        owner: root
        group: root
        mode: '0644'

    - name: Create .kube directory for ubuntu user (master only)
      file:
        path: /home/ubuntu/.kube
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Copy kubeconfig for ubuntu user (master only)
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/ubuntu/.kube/config
        remote_src: yes
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Generate join command for worker nodes
      shell: kubeadm token create --print-join-command
      register: join_command
      changed_when: false

    - name: Save join command to local file
      local_action:
        module: copy
        content: "{{ join_command.stdout_lines[0] }}"
        dest: "./{{ cluster_name }}-join-command.sh"
        mode: '0755'
      become: no

    - name: Install Flannel CNI plugin
      shell: |
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      when: not k8s_init_stat.stat.exists

    - name: Wait for all system pods to be ready
      shell: kubectl get pods -n kube-system --no-headers | grep -v Running | wc -l
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      register: pending_pods
      until: pending_pods.stdout == "0"
      retries: 30
      delay: 10
      changed_when: false
      become: false

    - name: Show system pods status (for debug)
      shell: kubectl get pods -n kube-system -o wide
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      register: pod_status
      changed_when: false
      become: false

    - name: Print pod status
      debug:
        var: pod_status.stdout_lines

- name: Join worker nodes to cluster
  hosts: "{{ cluster_name }}_workers"
  become: yes
  vars:
    cri_endpoint: "unix:///run/containerd/containerd.sock"
  tags:
    - cluster
  tasks:
    - name: Check if node is already part of cluster
      shell: kubectl get nodes | grep {{ ansible_hostname }} || true
      delegate_to: "{{ groups[cluster_name + '_master'][0] }}"
      register: node_status
      changed_when: false

    - name: Copy join command to worker nodes
      copy:
        src: "./{{ cluster_name }}-join-command.sh"
        dest: "/tmp/join-command.sh"
        mode: '0755'
      when: ansible_hostname not in node_status.stdout

    - name: Wait for containerd to be healthy before join
      shell: crictl --runtime-endpoint {{ cri_endpoint }} info
      register: cri_info
      retries: 10
      delay: 5
      until: cri_info.rc == 0
      changed_when: false

    - name: Join worker nodes to cluster
      shell: bash /tmp/join-command.sh
      when: ansible_hostname not in node_status.stdout
      register: join_result
      ignore_errors: yes

    - name: Show containerd logs if join failed
      shell: journalctl -xeu containerd | tail -n 40
      when: join_result is failed
      changed_when: false
      ignore_errors: true

    - name: Clean up join command file
      file:
        path: /tmp/join-command.sh
        state: absent

- name: Final cluster verification
  hosts: "{{ cluster_name }}_master"
  become: yes
  tags:
    - cluster
  tasks:
    - name: Wait for all nodes to be ready
      shell: kubectl get nodes --no-headers | grep -v Ready | wc -l
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      register: not_ready_nodes
      until: not_ready_nodes.stdout == "0"
      retries: 20
      delay: 15
      changed_when: false
      become: false

    - name: Display cluster status
      shell: kubectl get nodes -o wide
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      register: cluster_status
      changed_when: false
      become: false

    - name: Show cluster information
      debug:
        msg: "{{ cluster_status.stdout_lines }}"

    - name: Display cluster info
      shell: kubectl cluster-info
      environment:
        KUBECONFIG: /home/ubuntu/.kube/config
      register: cluster_info
      changed_when: false
      become: false

    - name: Show cluster info
      debug:
        msg: "{{ cluster_info.stdout_lines }}"

    - name: Create kubeconfig for regular user (optional)
      block:
        - name: Create user .kube directory
          file:
            path: "/home/{{ ansible_user }}/.kube"
            state: directory
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0755'
          when: ansible_user != "root"

        - name: Copy kubeconfig to user directory
          copy:
            src: /etc/kubernetes/admin.conf
            dest: "/home/{{ ansible_user }}/.kube/config"
            remote_src: yes
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            mode: '0644'
          when: ansible_user != "root"

- name: Clean up local files
  hosts: localhost
  connection: local
  become: no
  vars:
    cluster_name: "{{ cluster_name | default('k8scluster') }}"
  tags:
    - cluster
  tasks:
    - name: Remove temporary join command file
      file:
        path: "./{{ cluster_name }}-join-command.sh"
        state: absent
